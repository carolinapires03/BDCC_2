{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "814bd57a",
   "metadata": {},
   "source": [
    "# Big Data and Cloud Computing - Assignment #2\n",
    "\n",
    "Group AC\n",
    "\n",
    "- Bárbara Nóbrega Galiza - 202408654\n",
    "- Carolina Nunes Valente Pires - 202408704\n",
    "- Cláudia Oliveira -  202005668"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, stddev, when, isnan, ceil, coalesce, to_timestamp, countDistinct, collect_list, lit, mean, min, max, sum, first, udf\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from functools import reduce\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.types import IntegerType\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    mean_absolute_percentage_error, make_scorer\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"PYSPARK_PYTHON\"] = r\"C:\\Users\\claud\\AppData\\Local\\Programs\\Python\\Python39\\python.exe\"\n",
    "#os.environ[\"PYSPARK_DRIVER_PYTHON\"] = r\"C:\\Users\\claud\\AppData\\Local\\Programs\\Python\\Python39\\python.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ef83af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ICU Length of Stay Prediction\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bdbb55",
   "metadata": {},
   "source": [
    "This next chunk of code, differently from pandas, does not load the data immediatly to memory, it only builds a logical plan for the computation. However, it still take some time because it also does file discovery and infers schema from a sample (still an I/O operation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b658d693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CSV load time: 394.53 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "icustays = spark.read.csv(\"tables/ICUSTAYS.csv\", header=True, inferSchema=True)\n",
    "chartevents = spark.read.csv(\"tables/CHARTEVENTS_BIG.csv\", header=True, inferSchema=True)\n",
    "diagnoses = spark.read.csv('tables/DIAGNOSES_ICD.csv', header=True, inferSchema=True)\n",
    "diagnosis_names = spark.read.csv('tables/D_ICD_DIAGNOSES.csv', header=True, inferSchema=True)\n",
    "admissions = spark.read.csv('tables/ADMISSIONS.csv', header=True, inferSchema=True)\n",
    "patients = spark.read.csv('tables/PATIENTS.csv', header=True, inferSchema=True)\n",
    "items = spark.read.csv('tables/D_ITEMS.csv', header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total CSV load time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac514c",
   "metadata": {},
   "source": [
    "Checking if inferSchema worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a91f9994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ROW_ID: integer (nullable = true)\n",
      " |-- SUBJECT_ID: integer (nullable = true)\n",
      " |-- HADM_ID: integer (nullable = true)\n",
      " |-- ICUSTAY_ID: integer (nullable = true)\n",
      " |-- DBSOURCE: string (nullable = true)\n",
      " |-- FIRST_CAREUNIT: string (nullable = true)\n",
      " |-- LAST_CAREUNIT: string (nullable = true)\n",
      " |-- FIRST_WARDID: integer (nullable = true)\n",
      " |-- LAST_WARDID: integer (nullable = true)\n",
      " |-- INTIME: timestamp (nullable = true)\n",
      " |-- OUTTIME: timestamp (nullable = true)\n",
      " |-- LOS: double (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- ROW_ID: integer (nullable = true)\n",
      " |-- SUBJECT_ID: integer (nullable = true)\n",
      " |-- HADM_ID: integer (nullable = true)\n",
      " |-- ICUSTAY_ID: integer (nullable = true)\n",
      " |-- ITEMID: integer (nullable = true)\n",
      " |-- CHARTTIME: timestamp (nullable = true)\n",
      " |-- STORETIME: timestamp (nullable = true)\n",
      " |-- CGID: integer (nullable = true)\n",
      " |-- VALUE: string (nullable = true)\n",
      " |-- VALUENUM: double (nullable = true)\n",
      " |-- VALUEUOM: string (nullable = true)\n",
      " |-- WARNING: integer (nullable = true)\n",
      " |-- ERROR: integer (nullable = true)\n",
      " |-- RESULTSTATUS: string (nullable = true)\n",
      " |-- STOPPED: string (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- ROW_ID: integer (nullable = true)\n",
      " |-- SUBJECT_ID: integer (nullable = true)\n",
      " |-- HADM_ID: integer (nullable = true)\n",
      " |-- SEQ_NUM: integer (nullable = true)\n",
      " |-- ICD9_CODE: string (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- ROW_ID: integer (nullable = true)\n",
      " |-- SUBJECT_ID: integer (nullable = true)\n",
      " |-- HADM_ID: integer (nullable = true)\n",
      " |-- ADMITTIME: timestamp (nullable = true)\n",
      " |-- DISCHTIME: timestamp (nullable = true)\n",
      " |-- DEATHTIME: timestamp (nullable = true)\n",
      " |-- ADMISSION_TYPE: string (nullable = true)\n",
      " |-- ADMISSION_LOCATION: string (nullable = true)\n",
      " |-- DISCHARGE_LOCATION: string (nullable = true)\n",
      " |-- INSURANCE: string (nullable = true)\n",
      " |-- LANGUAGE: string (nullable = true)\n",
      " |-- RELIGION: string (nullable = true)\n",
      " |-- MARITAL_STATUS: string (nullable = true)\n",
      " |-- ETHNICITY: string (nullable = true)\n",
      " |-- EDREGTIME: timestamp (nullable = true)\n",
      " |-- EDOUTTIME: timestamp (nullable = true)\n",
      " |-- DIAGNOSIS: string (nullable = true)\n",
      " |-- HOSPITAL_EXPIRE_FLAG: integer (nullable = true)\n",
      " |-- HAS_CHARTEVENTS_DATA: integer (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- ROW_ID: integer (nullable = true)\n",
      " |-- SUBJECT_ID: integer (nullable = true)\n",
      " |-- GENDER: string (nullable = true)\n",
      " |-- DOB: timestamp (nullable = true)\n",
      " |-- DOD: timestamp (nullable = true)\n",
      " |-- DOD_HOSP: timestamp (nullable = true)\n",
      " |-- DOD_SSN: timestamp (nullable = true)\n",
      " |-- EXPIRE_FLAG: integer (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- ROW_ID: integer (nullable = true)\n",
      " |-- ITEMID: integer (nullable = true)\n",
      " |-- LABEL: string (nullable = true)\n",
      " |-- ABBREVIATION: string (nullable = true)\n",
      " |-- DBSOURCE: string (nullable = true)\n",
      " |-- LINKSTO: string (nullable = true)\n",
      " |-- CATEGORY: string (nullable = true)\n",
      " |-- UNITNAME: string (nullable = true)\n",
      " |-- PARAM_TYPE: string (nullable = true)\n",
      " |-- CONCEPTID: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(icustays.printSchema())\n",
    "print(chartevents.printSchema())\n",
    "print(diagnoses.printSchema())\n",
    "print(admissions.printSchema())\n",
    "print(patients.printSchema())\n",
    "print(items.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85f968",
   "metadata": {},
   "source": [
    "It appears to have worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de34671a",
   "metadata": {},
   "source": [
    "Now, let's remove unnecessary columns to make computation more efficient:\n",
    "\n",
    "Columns to be kept: \n",
    "\n",
    "- icustays: subject_id, hadm_id, icustay_id, intime, los\n",
    "- chartevents: subject_id, icustay_id, itemid, charttime, value\n",
    "- diagnoses: subject_id, hadm_id, seq_num, icd9_code\n",
    "- admissions: subject_id, hadm_id, deathtime\n",
    "- patients: subject_id, dob\n",
    "- items: itemid, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04f7f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "icustays = icustays.select(\"SUBJECT_ID\", \"HADM_ID\", \"ICUSTAY_ID\", \"INTIME\", \"LOS\")\n",
    "chartevents = chartevents.select(\"SUBJECT_ID\", \"ICUSTAY_ID\", \"ITEMID\", \"CHARTTIME\", \"VALUE\")\n",
    "diagnoses = diagnoses.select(\"SUBJECT_ID\", \"HADM_ID\", \"SEQ_NUM\", \"ICD9_CODE\")\n",
    "admissions = admissions.select(\"SUBJECT_ID\", \"HADM_ID\", \"DEATHTIME\")\n",
    "patients = patients.select(\"SUBJECT_ID\", \"DOB\")\n",
    "items = items.select(\"ITEMID\", \"LABEL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5803ca59",
   "metadata": {},
   "source": [
    "Tables description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0a5ea7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-----------------+\n",
      "|summary|        SUBJECT_ID|           HADM_ID|        ICUSTAY_ID|              LOS|\n",
      "+-------+------------------+------------------+------------------+-----------------+\n",
      "|  count|             61532|             61532|             61532|            61522|\n",
      "|   mean|  33888.6059123708| 149954.4706494182|249962.71024832607|4.917971580897899|\n",
      "| stddev|28127.690913330127|28898.895903803314|  28890.5748673448| 9.63878425110918|\n",
      "|    min|                 2|            100001|            200001|           1.0E-4|\n",
      "|    max|             99999|            199999|            299999|         173.0725|\n",
      "+-------+------------------+------------------+------------------+-----------------+\n",
      "\n",
      "None\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|        SUBJECT_ID|           HADM_ID|           SEQ_NUM|         ICD9_CODE|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|            651047|            651047|            651000|            651000|\n",
      "|   mean| 38971.15975805126|150017.74481873045|7.9138356374807985|25390.601889657988|\n",
      "| stddev|29372.198841354115|28878.068648016237| 6.072633414653878|28365.292859393663|\n",
      "|    min|                 2|            100001|                 1|              0030|\n",
      "|    max|             99999|            199999|                39|             V9103|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n",
      "None\n",
      "+-------+------------------+------------------+\n",
      "|summary|        SUBJECT_ID|           HADM_ID|\n",
      "+-------+------------------+------------------+\n",
      "|  count|             58976|             58976|\n",
      "|   mean|  33755.5832881172| 149970.8095835594|\n",
      "| stddev|28092.726225170416|28883.095213439887|\n",
      "|    min|                 2|            100001|\n",
      "|    max|             99999|            199999|\n",
      "+-------+------------------+------------------+\n",
      "\n",
      "None\n",
      "+-------+-----------------+\n",
      "|summary|       SUBJECT_ID|\n",
      "+-------+-----------------+\n",
      "|  count|            46520|\n",
      "|   mean|34425.77287188306|\n",
      "| stddev|28330.40034288402|\n",
      "|    min|                2|\n",
      "|    max|            99999|\n",
      "+-------+-----------------+\n",
      "\n",
      "None\n",
      "+-------+-----------------+--------------------+\n",
      "|summary|           ITEMID|               LABEL|\n",
      "+-------+-----------------+--------------------+\n",
      "|  count|            12487|               12484|\n",
      "|   mean|72311.93008729078|                NULL|\n",
      "| stddev|88455.38177113343|                NULL|\n",
      "|    min|                1|\"MULTIVIT \"\"BANAN...|\n",
      "|    max|           228647|          zzzzz Date|\n",
      "+-------+-----------------+--------------------+\n",
      "\n",
      "None\n",
      "Total time: 6.43 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print(icustays.describe().show())\n",
    "print(diagnoses.describe().show())\n",
    "print(admissions.describe().show())\n",
    "print(patients.describe().show())\n",
    "print(items.describe().show())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40667c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|        SUBJECT_ID|        ICUSTAY_ID|            ITEMID|             VALUE|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|         330712483|         330414954|         330712483|         328641134|\n",
      "|   mean|31300.602687235125| 250195.2241299678|  75668.3713958992| 77.32892799832888|\n",
      "| stddev|27009.800314481552|28732.974394786408|104358.27040968732|2130.3461492287856|\n",
      "|    min|                 2|            200001|                 1|             \\tCDI|\n",
      "|    max|             99999|            299999|            228451|              ~150|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n",
      "None\n",
      "Total time: 610.36 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print(chartevents.describe().show())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998053e4",
   "metadata": {},
   "source": [
    "The time necessary to apply describe, that is, calculate summary statistics is much bigger for the chartevents alone then for all the other tables, which is expected due to its size. It confirms once again chartevents will be the bottlencek of this ML pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21feb6ae",
   "metadata": {},
   "source": [
    "## Data preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d38daa1",
   "metadata": {},
   "source": [
    "### Filtering based on window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf61cda9",
   "metadata": {},
   "source": [
    "Filter the chartevents to only have events from the first 2 days of the ICU stay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7358442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0.24 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "icustays = icustays.withColumn(\"INTIME\", to_timestamp(\"INTIME\"))\n",
    "chartevents = chartevents.withColumn(\"CHARTTIME\", to_timestamp(\"CHARTTIME\"))\n",
    "\n",
    "merged = chartevents.join(\n",
    "    icustays.select(\"ICUSTAY_ID\", \"INTIME\", \"LOS\", \"HADM_ID\"),\n",
    "    on=\"ICUSTAY_ID\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "merged = merged.withColumn(\n",
    "    \"DAYS_FROM_INTIME\",\n",
    "    (col(\"CHARTTIME\").cast(\"long\") - col(\"INTIME\").cast(\"long\")) / 86400.0\n",
    ")\n",
    "\n",
    "merged = merged.withColumn(\n",
    "    \"DAYS_FROM_INTIME\",\n",
    "    when(col(\"DAYS_FROM_INTIME\") < 0, lit(0)).otherwise(col(\"DAYS_FROM_INTIME\"))\n",
    ")\n",
    "\n",
    "chartevents_2nd_day = merged.filter(col(\"DAYS_FROM_INTIME\") < 2).drop(\"DAYS_FROM_INTIME\", \"CHARTTIME\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7267043",
   "metadata": {},
   "source": [
    "Fast because just the DAG was built. Real loading of the csv, merging, filtering and counting computation happens in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5f806f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117272707\n",
      "Total time: 300.68 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print(chartevents_2nd_day.count())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1365e0",
   "metadata": {},
   "source": [
    "From 330M to 117M, a meagninful reduction, however 1/3 is still a good amount of  data. It means 1/3 of the chartevents were recorded within the first 48h of the ICU stay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec692db",
   "metadata": {},
   "source": [
    "Checking columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9254c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ICUSTAY_ID', 'SUBJECT_ID', 'ITEMID', 'VALUE', 'INTIME', 'LOS', 'HADM_ID']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chartevents_2nd_day.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b57cb",
   "metadata": {},
   "source": [
    "Since our goal is to predict the LOS of the patient after they stayed there for 2 days, it makes no sense to consider data of patients that died within the first 48h. Therefore, we first cut these patients' stays, then compute the top 5 most common items for the remainig ICU stays. Following this order makes computation more efficient, since we will only compute the top 5 based on a smaller number of chartevents, and assures unbiased results (top 5 are calculated only with valid ICU stays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56395306",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = chartevents_2nd_day.join(\n",
    "    admissions.select(\"HADM_ID\", \"DEATHTIME\"),\n",
    "    on=\"HADM_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "joined = joined.withColumn(\"DEATHTIME\", to_timestamp(\"DEATHTIME\"))\n",
    "joined = joined.withColumn(\"INTIME\", to_timestamp(\"INTIME\"))\n",
    "\n",
    "joined = joined.withColumn(\n",
    "    \"HOURS_TO_DEATH\",\n",
    "    (col(\"DEATHTIME\").cast(\"double\") - col(\"INTIME\").cast(\"double\")) / 3600.0\n",
    ")\n",
    "\n",
    "alive_48h = joined.filter(\n",
    "    col(\"DEATHTIME\").isNull() |\n",
    "    (col(\"HOURS_TO_DEATH\") >= 48)\n",
    ").drop(\"DEATHTIME\", \"INTIME\", \"HOURS_TO_DEATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a501fed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HADM_ID', 'ICUSTAY_ID', 'SUBJECT_ID', 'ITEMID', 'VALUE', 'LOS']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alive_48h.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d65a40fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114801472\n",
      "Total time: 281.26 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print(alive_48h.count())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718e6f94",
   "metadata": {},
   "source": [
    "About 3M rows were removed, with the total time being very close to when the removal was of 2/3 (~200M rows) of the data. This shows that in Spark, performance is driven more by the complexity of operations (in this case both used joins, colum creation with math and filters), rather than just the number of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6bc266",
   "metadata": {},
   "source": [
    "### Adding items (chartevents) as columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d15321",
   "metadata": {},
   "source": [
    "Now, we keep only the ICU stays that have the top 5 most common items that will be used for modeling. Since there are multiple items and there will be one or more columns based on each of them, the goal here is to limit the number of columns in the dataset fed to the model. By picking the top 5, we minimize the number of nulls we will have when creating the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "810517c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------------------------+\n",
      "|ITEMID|count  |LABEL                      |\n",
      "+------+-------+---------------------------+\n",
      "|211   |1542598|Heart Rate                 |\n",
      "|742   |1336371|calprevflg                 |\n",
      "|618   |1306673|Respiratory Rate           |\n",
      "|646   |1299691|SpO2                       |\n",
      "|212   |1270531|Heart Rhythm               |\n",
      "|161   |1242103|Ectopy Type                |\n",
      "|220045|1195309|Heart Rate                 |\n",
      "|220210|1184042|Respiratory Rate           |\n",
      "|220277|1154033|O2 saturation pulseoxymetry|\n",
      "|128   |1148774|Code Status                |\n",
      "|550   |1142485|Precautions                |\n",
      "|1125  |1089363|Service Type               |\n",
      "|159   |956154 |Ectopy Frequency           |\n",
      "|220048|884684 |Heart Rhythm               |\n",
      "|227969|822776 |Safety Measures_U_1        |\n",
      "+------+-------+---------------------------+\n",
      "\n",
      "Total time: 294.61 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "top_items = alive_48h.groupBy(\"ITEMID\").count().orderBy(col(\"count\").desc()).limit(15)\n",
    "\n",
    "top_items_with_labels = top_items.join(\n",
    "    items.select(\"ITEMID\", \"LABEL\"),\n",
    "    on=\"ITEMID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "top_items_with_labels.orderBy(col(\"count\").desc()).show(truncate=False)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edcbab2",
   "metadata": {},
   "source": [
    "From these, we can see 742 is not actually a measurament but a flag, so we cannot use it. We need also to inspect if the remaining items are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9a0106c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|       VALUE|\n",
      "+------------+\n",
      "|Normal Sinus|\n",
      "|Normal Sinus|\n",
      "|    AV Paced|\n",
      "|Normal Sinus|\n",
      "|Normal Sinus|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alive_48h.filter(col(\"ITEMID\") == 212).select(\"VALUE\").show(n=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc202c",
   "metadata": {},
   "source": [
    "After testing for all codes, we see the only numerical ones in the top 15 are Heart Rate (212 and 220045), Respiratory Rate (618 and 220210), and Sp02/O2 saturation pulseoxymetry (646 and 220277). The other two valid measurements are Heart Rhythm (212 and 220048) and Ectopy Type (161), but they are categorical. We could create one-hot columns for each value then compute the frequency for each stay. However, we inspected and saw there are different sets of categories for each ID and a very big amount (more than 20) of distintic values for Heart Rhythm. The same happens for Ectopy Type. Therefore we chose to only use the numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a4eb2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|VALUE           |count  |\n",
      "+----------------+-------+\n",
      "|None            |1059239|\n",
      "|PVC's           |134907 |\n",
      "|PAC's           |43953  |\n",
      "|Vent. Bigeminy  |1443   |\n",
      "|NULL            |974    |\n",
      "|Atrial Bigeminy |421    |\n",
      "|Vent. Trigeminy |331    |\n",
      "|Nod/Junc Escape |312    |\n",
      "|PNC's           |252    |\n",
      "|Fusion Beats    |112    |\n",
      "|Vent. Escape    |85     |\n",
      "|Atrial Trigeminy|32     |\n",
      "|V Quadrigeminy  |22     |\n",
      "|Nodal Bigeminy  |18     |\n",
      "|Nodal Trigeminy |2      |\n",
      "+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alive_48h.filter(col(\"ITEMID\") == 161).groupBy(\"VALUE\").count().orderBy(\"count\", ascending=False).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6558d5f",
   "metadata": {},
   "source": [
    "Filtering based on the selected items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5d0c064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7661849\n",
      "Total time: 240.73 seconds\n"
     ]
    }
   ],
   "source": [
    "items_filtered = alive_48h.filter(col(\"ITEMID\").isin([211, 618, 646, 220045, 220210, 220277])).filter(col(\"VALUE\").isNotNull())\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(items_filtered.count())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd7790",
   "metadata": {},
   "source": [
    "The data reduced a lot but is still big, with 7.6M rows. Only applying filter was 20s quicker then applying joins and filters like before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e8874",
   "metadata": {},
   "source": [
    "Normalize column names so they can be grouped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee22ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_filtered = items_filtered.join(\n",
    "    items.select(\"ITEMID\", \"LABEL\"),\n",
    "    on=\"ITEMID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "items_filtered = items_filtered.replace({\n",
    "    \"O2 saturation pulseoxymetry\": \"SpO2\",\n",
    "}, subset=[\"LABEL\"])\n",
    "\n",
    "# items_filtered.filter(col(\"ITEMID\") == 220277).select(\"LABEL\").distinct().show() -> worked, no need to run everytime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d57ddb",
   "metadata": {},
   "source": [
    "The current dataset we have is one row by chartevent, so it has duplicated ICU stays. So, to check how much unique ICU stays we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b54ef0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58113"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_filtered.select(\"ICUSTAY_ID\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a64001",
   "metadata": {},
   "source": [
    "It is a much smaller number, but still reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988257c",
   "metadata": {},
   "source": [
    "Pivoting the data to get one row by ICU stay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d37c01c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 232.59 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "num_agg_long = items_filtered.groupBy(\"ICUSTAY_ID\", \"LABEL\").agg(\n",
    "    mean(\"VALUE\").alias(\"val_mean\"),\n",
    "    min(\"VALUE\").alias(\"val_min\"),\n",
    "    max(\"VALUE\").alias(\"val_max\")\n",
    ")\n",
    "\n",
    "icu_features = num_agg_long.groupBy(\"ICUSTAY_ID\").pivot(\"LABEL\").agg(\n",
    "    first(\"val_mean\").alias(\"mean\"),\n",
    "    first(\"val_min\").alias(\"min\"),\n",
    "    first(\"val_max\").alias(\"max\")\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f71a07b",
   "metadata": {},
   "source": [
    "Instead of computing twice by calling show() and then count(), we use cache so the computation can be triggered just once and run in half of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63d15b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#icu_features.unpersist()\n",
    "\n",
    "if not icu_features.is_cached:\n",
    "    icu_features.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb4897bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+--------------+--------------+---------------------+--------------------+--------------------+-----------------+--------+--------+\n",
      "|ICUSTAY_ID|  Heart Rate_mean|Heart Rate_min|Heart Rate_max|Respiratory Rate_mean|Respiratory Rate_min|Respiratory Rate_max|        SpO2_mean|SpO2_min|SpO2_max|\n",
      "+----------+-----------------+--------------+--------------+---------------------+--------------------+--------------------+-----------------+--------+--------+\n",
      "|    200166|78.51428571428572|            71|            90|   14.098591549295774|                  10|                   7|            100.0|     100|     100|\n",
      "|    200379|76.22058823529412|            69|            91|   12.956521739130435|                  10|                   9|            97.25|     100|      99|\n",
      "|    200625|81.20689655172414|           105|            85|   20.053571428571427|                  13|                  33|95.64814814814815|     100|      99|\n",
      "|    200687|            143.5|           137|           150|                 NULL|                NULL|                NULL|             NULL|    NULL|    NULL|\n",
      "|    200718|71.74418604651163|            59|            93|                 14.4|                  10|                   9|97.04761904761905|      95|      99|\n",
      "+----------+-----------------+--------------+--------------+---------------------+--------------------+--------------------+-----------------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "58113\n",
      "Total time: 238.30 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "icu_features.show(5)\n",
    "print(icu_features.count())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aeefe2",
   "metadata": {},
   "source": [
    "Now, we can see there are some null values, due to performing an outer join. So now we check how many rows have any missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc31e49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7740"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_rows = icu_features.filter(\n",
    "    reduce(lambda a, b: a | b, (col(c).isNull() for c in icu_features.columns))\n",
    ")\n",
    "\n",
    "null_rows.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4420546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------------+--------------+---------------------+--------------------+--------------------+---------+--------+--------+\n",
      "|ICUSTAY_ID|Heart Rate_mean|Heart Rate_min|Heart Rate_max|Respiratory Rate_mean|Respiratory Rate_min|Respiratory Rate_max|SpO2_mean|SpO2_min|SpO2_max|\n",
      "+----------+---------------+--------------+--------------+---------------------+--------------------+--------------------+---------+--------+--------+\n",
      "|         0|              0|             0|             0|                 7728|                7712|                7712|     7699|    7699|    7699|\n",
      "+----------+---------------+--------------+--------------+---------------------+--------------------+--------------------+---------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "icu_features.select([count(when(col(c).isNull(), c)).alias(c) for c in icu_features.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7cd3c9",
   "metadata": {},
   "source": [
    "Here we see we have 7740 rows with missing values, which represents about 13% of total data. Dropping would mean losing an import amount of data, and we can see we would lose heart rate information, since there are no missing values in those columns. Therefore, we will perform imputation. We will use the median because it is more robust to outliers. Since these measuraments are often taken automatically and in the same machine, their absence might not be related to the patient's state, so it is safer to imput values. Also, since our focus for this project is performance rather then the results themselves, it becomes more acceptable to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db625e41",
   "metadata": {},
   "source": [
    "Casting to double since some columns had type string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "686b2207",
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_features = icu_features.withColumn(\"Heart Rate_min\", col(\"Heart Rate_min\").cast(\"double\"))\n",
    "icu_features = icu_features.withColumn(\"Heart Rate_max\", col(\"Heart Rate_max\").cast(\"double\"))\n",
    "icu_features = icu_features.withColumn(\"Respiratory Rate_min\", col(\"Respiratory Rate_min\").cast(\"double\"))\n",
    "icu_features = icu_features.withColumn(\"Respiratory Rate_max\", col(\"Respiratory Rate_max\").cast(\"double\"))\n",
    "icu_features = icu_features.withColumn(\"SpO2_min\", col(\"SpO2_min\").cast(\"double\"))\n",
    "icu_features = icu_features.withColumn(\"SpO2_max\", col(\"SpO2_max\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ee3001b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------------+--------------+---------------------+--------------------+--------------------+---------+--------+--------+\n",
      "|ICUSTAY_ID|Heart Rate_mean|Heart Rate_min|Heart Rate_max|Respiratory Rate_mean|Respiratory Rate_min|Respiratory Rate_max|SpO2_mean|SpO2_min|SpO2_max|\n",
      "+----------+---------------+--------------+--------------+---------------------+--------------------+--------------------+---------+--------+--------+\n",
      "|         0|              0|             0|             0|                    0|                   0|                   0|        0|       0|       0|\n",
      "+----------+---------------+--------------+--------------+---------------------+--------------------+--------------------+---------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_to_impute = [\"Respiratory Rate_mean\", \"Respiratory Rate_min\", \"Respiratory Rate_max\",\n",
    "                  \"SpO2_mean\", \"SpO2_min\", \"SpO2_max\"]\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=cols_to_impute,\n",
    "    outputCols=cols_to_impute,  \n",
    "    strategy=\"median\"\n",
    ")\n",
    "\n",
    "icu_features_imputed = imputer.fit(icu_features).transform(icu_features)\n",
    "\n",
    "icu_features_imputed.select([count(when(col(c).isNull(), c)).alias(c) for c in icu_features.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5488a88c",
   "metadata": {},
   "source": [
    "### Adding disease (diagnose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e4472",
   "metadata": {},
   "source": [
    "First, we need to check how many different diagnoses (ICD9 codes) exist, to decide whether we need to group them or not before transforming them in columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cbf9e5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6985"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnoses.select(\"ICD9_CODE\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fb8fd",
   "metadata": {},
   "source": [
    "~7k is a very high number, so it is better to group these codes. We will use the chapter division defined by the [ICD9 code guidelines](https://www.ama-assn.org/sites/ama-assn.org/files/corp/media-browser/public/cpt/icd9cm_coding_guidelines_08_09_full_0.pdf), which will lead to 19 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "926fd587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+---------------------+-----+------+-------+-----------+-----------+---------+-------------+--------------------+----+---------------+----------+---------+--------+----------------+------+------+\n",
      "|HADM_ID|Infectious|Neoplasms|Endocrine_Nutritional|Blood|Mental|Nervous|Circulatory|Respiratory|Digestive|Genitourinary|Pregnancy_Childbirth|Skin|Musculoskeletal|Congenital|Perinatal|Symptoms|Injury_Poisoning|V_Code|E_Code|\n",
      "+-------+----------+---------+---------------------+-----+------+-------+-----------+-----------+---------+-------------+--------------------+----+---------------+----------+---------+--------+----------------+------+------+\n",
      "| 192988|         0|        0|                    0|    0|     0|      0|          1|          0|        0|            0|                   0|   0|              0|         0|        0|       0|               0|     0|     0|\n",
      "| 124411|         0|        0|                    1|    0|     0|      0|          1|          0|        0|            1|                   0|   0|              0|         0|        0|       1|               0|     0|     0|\n",
      "| 120988|         0|        0|                    0|    0|     0|      0|          0|          0|        0|            0|                   0|   0|              0|         0|        1|       0|               0|     1|     0|\n",
      "| 112020|         0|        1|                    1|    1|     0|      0|          0|          1|        0|            1|                   0|   0|              0|         0|        0|       0|               0|     0|     0|\n",
      "| 132406|         0|        0|                    0|    0|     0|      0|          0|          0|        0|            0|                   0|   0|              0|         0|        0|       0|               0|     1|     0|\n",
      "| 124967|         0|        0|                    0|    0|     0|      1|          0|          0|        0|            0|                   0|   0|              0|         1|        1|       0|               0|     1|     0|\n",
      "| 133948|         0|        0|                    0|    0|     1|      0|          0|          0|        0|            0|                   0|   0|              0|         0|        0|       0|               1|     0|     1|\n",
      "| 173508|         0|        0|                    0|    0|     0|      0|          0|          0|        0|            0|                   0|   0|              0|         1|        0|       0|               0|     1|     0|\n",
      "| 160767|         0|        0|                    0|    0|     0|      0|          0|          0|        0|            0|                   0|   0|              0|         0|        1|       0|               0|     1|     0|\n",
      "| 127444|         0|        0|                    0|    0|     0|      0|          1|          0|        0|            0|                   0|   0|              0|         0|        0|       0|               0|     0|     0|\n",
      "| 160235|         0|        0|                    1|    1|     0|      0|          1|          0|        0|            0|                   0|   0|              0|         0|        0|       0|               0|     0|     0|\n",
      "| 135965|         0|        0|                    0|    0|     0|      0|          0|          0|        0|            0|                   0|   0|              0|         0|        0|       0|               0|     1|     0|\n",
      "| 145011|         1|        0|                    1|    1|     0|      0|          1|          1|        0|            0|                   0|   0|              0|         0|        0|       0|               0|     0|     0|\n",
      "| 135423|         0|        0|                    0|    0|     0|      0|          1|          0|        0|            0|                   0|   0|              0|         0|        0|       0|               1|     1|     1|\n",
      "| 144907|         0|        0|                    0|    0|     1|      0|          1|          0|        0|            0|                   0|   0|              0|         0|        0|       0|               1|     0|     0|\n",
      "| 147280|         0|        0|                    0|    0|     0|      0|          0|          0|        0|            0|                   0|   0|              0|         0|        1|       0|               0|     1|     0|\n",
      "| 192401|         1|        1|                    0|    1|     0|      0|          1|          1|        1|            1|                   0|   0|              1|         0|        0|       0|               1|     0|     0|\n",
      "| 187693|         0|        0|                    1|    0|     0|      0|          1|          0|        0|            0|                   0|   0|              0|         0|        0|       1|               1|     0|     0|\n",
      "| 197148|         0|        0|                    0|    0|     0|      0|          0|          0|        0|            0|                   0|   0|              0|         0|        1|       0|               0|     1|     0|\n",
      "| 174229|         0|        1|                    0|    1|     0|      0|          1|          0|        1|            1|                   0|   0|              0|         0|        0|       0|               0|     0|     0|\n",
      "+-------+----------+---------+---------------------+-----+------+-------+-----------+-----------+---------+-------------+--------------------+----+---------------+----------+---------+--------+----------------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groups = [\n",
    "    (\"Infectious\", 1, 139),\n",
    "    (\"Neoplasms\", 140, 239),\n",
    "    (\"Endocrine_Nutritional\", 240, 279),\n",
    "    (\"Blood\", 280, 289),\n",
    "    (\"Mental\", 290, 319),\n",
    "    (\"Nervous\", 320, 389),\n",
    "    (\"Circulatory\", 390, 459),\n",
    "    (\"Respiratory\", 460, 519),\n",
    "    (\"Digestive\", 520, 579),\n",
    "    (\"Genitourinary\", 580, 629),\n",
    "    (\"Pregnancy_Childbirth\", 630, 679),\n",
    "    (\"Skin\", 680, 709),\n",
    "    (\"Musculoskeletal\", 710, 739),\n",
    "    (\"Congenital\", 740, 759),\n",
    "    (\"Perinatal\", 760, 779),\n",
    "    (\"Symptoms\", 780, 799),\n",
    "    (\"Injury_Poisoning\", 800, 999),\n",
    "]\n",
    "\n",
    "def icd9_first3_num(icd9):\n",
    "    if icd9 is None:\n",
    "        return None\n",
    "    if icd9.startswith('V') or icd9.startswith('E'):\n",
    "        return None\n",
    "    first3 = icd9[:3]\n",
    "    try:\n",
    "        return int(first3)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "\n",
    "icd9_first3_num_udf = udf(icd9_first3_num, IntegerType())\n",
    "\n",
    "df = diagnoses.withColumn(\"ICD9_FIRST3\", icd9_first3_num_udf(col(\"ICD9_CODE\")))\n",
    "\n",
    "# V and E codes flags remain the same\n",
    "df = df.withColumn(\"V_Code\", when(col(\"ICD9_CODE\").startswith(\"V\"), 1).otherwise(0))\n",
    "df = df.withColumn(\"E_Code\", when(col(\"ICD9_CODE\").startswith(\"E\"), 1).otherwise(0))\n",
    "\n",
    "# Create group columns using first 3 digits numeric\n",
    "for group_name, start, end in groups:\n",
    "    df = df.withColumn(\n",
    "        group_name,\n",
    "        when((col(\"ICD9_FIRST3\") >= start) & (col(\"ICD9_FIRST3\") <= end), 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "agg_cols = [max(group_name).alias(group_name) for group_name, _, _ in groups] + [\n",
    "    max(\"V_Code\").alias(\"V_Code\"),\n",
    "    max(\"E_Code\").alias(\"E_Code\"),\n",
    "]\n",
    "\n",
    "df_grouped = df.groupBy(\"HADM_ID\").agg(*agg_cols)\n",
    "\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14b7cf7",
   "metadata": {},
   "source": [
    "Now, we merge with the icu_features_imputed and items_filtered dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6dd4fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_info = items_filtered.select('HADM_ID', 'ICUSTAY_ID', 'SUBJECT_ID', 'LOS').distinct()\n",
    "final_df = icu_features_imputed.join(patient_info, on=\"ICUSTAY_ID\", how=\"inner\")\n",
    "final_df = final_df.join(df_grouped, on=\"HADM_ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c3e262",
   "metadata": {},
   "source": [
    "Again, let`s cache to perform two computations over the same df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4fbf65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df.unpersist()\n",
    "\n",
    "if not final_df.is_cached:\n",
    "    final_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6d04aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58113\n",
      "+-------+----------+---------------+--------------+--------------+---------------------+--------------------+--------------------+---------+--------+--------+----------+---+----------+---------+---------------------+-----+------+-------+-----------+-----------+---------+-------------+--------------------+----+---------------+----------+---------+--------+----------------+------+------+\n",
      "|HADM_ID|ICUSTAY_ID|Heart Rate_mean|Heart Rate_min|Heart Rate_max|Respiratory Rate_mean|Respiratory Rate_min|Respiratory Rate_max|SpO2_mean|SpO2_min|SpO2_max|SUBJECT_ID|LOS|Infectious|Neoplasms|Endocrine_Nutritional|Blood|Mental|Nervous|Circulatory|Respiratory|Digestive|Genitourinary|Pregnancy_Childbirth|Skin|Musculoskeletal|Congenital|Perinatal|Symptoms|Injury_Poisoning|V_Code|E_Code|\n",
      "+-------+----------+---------------+--------------+--------------+---------------------+--------------------+--------------------+---------+--------+--------+----------+---+----------+---------+---------------------+-----+------+-------+-----------+-----------+---------+-------------+--------------------+----+---------------+----------+---------+--------+----------------+------+------+\n",
      "|      0|         0|              0|             0|             0|                    0|                   0|                   0|        0|       0|       0|         0|  0|         0|        0|                    0|    0|     0|      0|          0|          0|        0|            0|                   0|   0|              0|         0|        0|       0|               0|     0|     0|\n",
      "+-------+----------+---------------+--------------+--------------+---------------------+--------------------+--------------------+---------+--------+--------+----------+---+----------+---------+---------------------+-----+------+-------+-----------+-----------+---------+-------------+--------------------+----+---------------+----------+---------+--------+----------------+------+------+\n",
      "\n",
      "None\n",
      "Total time: 254.62 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print(final_df.count())\n",
    "print(final_df.select([count(when(col(c).isNull(), c)).alias(c) for c in final_df.columns]).show())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b504b",
   "metadata": {},
   "source": [
    "These two joins were faster then some previous operations using joins and filters, possibly due to lower amount of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e683578",
   "metadata": {},
   "source": [
    "The data is now ready for modeling and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a0ab42",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8aade065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' los_values = icustays.select(\"LOS\").dropna().rdd.flatMap(lambda x: x).collect() # rdd: more efficient \\n\\nmax_los = int(np.ceil(max(los_values)))\\n\\nprint(max(los_values))\\n\\nplt.figure(figsize=(16, 6))\\nplt.hist(los_values, bins=np.arange(0, max_los + 1, 1), edgecolor=\\'black\\', color=\\'skyblue\\')\\nplt.title(\"LOS distribution in the MIMIC dataset\")\\nplt.ylabel(\"Count\")\\nplt.xlabel(\"LOS (Days)\")\\nplt.xticks(np.arange(0, max_los + 1, step=1), rotation=90)\\nplt.tight_layout()\\nplt.show() '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" los_values = icustays.select(\"LOS\").dropna().rdd.flatMap(lambda x: x).collect() # rdd: more efficient \n",
    "\n",
    "max_los = int(np.ceil(max(los_values)))\n",
    "\n",
    "print(max(los_values))\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.hist(los_values, bins=np.arange(0, max_los + 1, 1), edgecolor='black', color='skyblue')\n",
    "plt.title(\"LOS distribution in the MIMIC dataset\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"LOS (Days)\")\n",
    "plt.xticks(np.arange(0, max_los + 1, step=1), rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show() \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7334515",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9bd810",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e07127",
   "metadata": {},
   "source": [
    "We first convert our final dataset to a Pandas DataFrame, since the initial data processing significantly reduced its size. This allows us to efficiently explore the use of scikit-learn pipelines and joblib in this modeling phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "89c862db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = final_df.toPandas()\n",
    "\n",
    "X = df_model.drop(columns=['LOS', 'HADM_ID', 'ICUSTAY_ID'])\n",
    "y = df_model['LOS']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361962d4",
   "metadata": {},
   "source": [
    "To maintain cleaner and more organized code, here are our evaluation and training functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b653fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_store(model_name, y_test, y_pred, duration, mode):\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Mode\": mode,\n",
    "        \"Time (s)\": round(duration, 2),\n",
    "        \"MAE\": round(mae, 2),\n",
    "        \"MSE\": round(mse, 2),\n",
    "        \"RMSE\": round(rmse, 2),\n",
    "        \"R²\": round(r2, 2),\n",
    "        \"MAPE (%)\": round(mape, 2)\n",
    "    })\n",
    "\n",
    "def run_manual(model_name, model):\n",
    "    start = time.time()\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    duration = time.time() - start\n",
    "\n",
    "    evaluate_and_store(model_name, y_test, y_pred, duration, \"Manual\")\n",
    "\n",
    "def run_pipeline_no_grid(model_name, model):\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "\n",
    "    start = time.time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    duration = time.time() - start\n",
    "\n",
    "    evaluate_and_store(model_name, y_test, y_pred, duration, \"Pipeline (no GridSearch)\")\n",
    "\n",
    "def run_pipeline_grid(model_name, model, param_grid, n_jobs_val):\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scorer,\n",
    "        cv=kf,\n",
    "        n_jobs=n_jobs_val\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    grid.fit(X_train, y_train)\n",
    "    duration = time.time() - start\n",
    "\n",
    "    y_pred = grid.best_estimator_.predict(X_test)\n",
    "    label = f\"Pipeline + GridSearch (n_jobs={n_jobs_val})\"\n",
    "    evaluate_and_store(model_name, y_test, y_pred, duration, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e5d242",
   "metadata": {},
   "source": [
    "To evaluate both performance and execution time using joblib and Pipeline, we test four different regression models, some more simpler than others. Also, we're testing 3 variations: No pipeline, no parallel; Pipeline, no parallel (n_jobs=1); Pipeline + GridSearch + joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b52206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_info = [\n",
    "    (\"Linear Regression\", LinearRegression(), {}),\n",
    "    (\"Ridge Regression\", Ridge(), {\n",
    "        'regressor__alpha': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "        'regressor__solver': ['auto'],\n",
    "        'regressor__max_iter': [1000]\n",
    "    }),\n",
    "    (\"Decision Tree\", DecisionTreeRegressor(random_state=42), {\n",
    "        'regressor__max_depth': [5, 10, None],\n",
    "        'regressor__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    \n",
    "    (\"Random Forest\", RandomForestRegressor(random_state=42), {\n",
    "        'regressor__n_estimators': [100],\n",
    "        'regressor__max_depth': [10, None]\n",
    "    }),\n",
    "    (\"XGBoost\", XGBRegressor(random_state=42, verbosity=0), {\n",
    "        'regressor__n_estimators': [100],\n",
    "        'regressor__learning_rate': [0.1],\n",
    "        'regressor__max_depth': [3]\n",
    "    })\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a828acc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary Table ===\n",
      "            Model                              Mode  Time (s)  MAE   MSE  RMSE   R²  MAPE (%)\n",
      "Linear Regression                            Manual      0.16 4.45 81.60  9.03 0.20    450.08\n",
      "Linear Regression          Pipeline (no GridSearch)      0.10 4.45 81.60  9.03 0.20    450.08\n",
      "Linear Regression  Pipeline + GridSearch (n_jobs=1)      0.52 4.45 81.60  9.03 0.20    450.08\n",
      "Linear Regression Pipeline + GridSearch (n_jobs=-1)      6.08 4.45 81.60  9.03 0.20    450.08\n",
      " Ridge Regression                            Manual      0.08 4.44 81.61  9.03 0.20    446.92\n",
      " Ridge Regression          Pipeline (no GridSearch)      0.08 4.44 81.61  9.03 0.20    446.92\n",
      " Ridge Regression  Pipeline + GridSearch (n_jobs=1)      1.76 4.41 81.87  9.05 0.20    430.96\n",
      " Ridge Regression Pipeline + GridSearch (n_jobs=-1)      3.37 4.41 81.87  9.05 0.20    430.96\n",
      "    Random Forest                            Manual     85.99 3.81 63.42  7.96 0.38    183.56\n",
      "    Random Forest          Pipeline (no GridSearch)     82.59 3.81 63.42  7.96 0.38    183.56\n",
      "    Random Forest  Pipeline + GridSearch (n_jobs=1)    490.37 3.67 61.53  7.84 0.39    181.50\n",
      "    Random Forest Pipeline + GridSearch (n_jobs=-1)    214.70 3.67 61.53  7.84 0.39    181.50\n",
      "          XGBoost                            Manual      0.45 3.70 62.15  7.88 0.39    201.82\n",
      "          XGBoost          Pipeline (no GridSearch)      0.66 3.70 62.15  7.88 0.39    201.82\n",
      "          XGBoost  Pipeline + GridSearch (n_jobs=1)      2.71 3.73 60.62  7.79 0.40    259.17\n",
      "          XGBoost Pipeline + GridSearch (n_jobs=-1)      1.75 3.73 60.62  7.79 0.40    259.17\n"
     ]
    }
   ],
   "source": [
    "for model_name, model, param_grid in models_info:\n",
    "    run_manual(model_name, model)\n",
    "    run_pipeline_no_grid(model_name, model)\n",
    "    run_pipeline_grid(model_name, model, param_grid, n_jobs_val=1)\n",
    "    run_pipeline_grid(model_name, model, param_grid, n_jobs_val=-1)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Summary Table ===\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3b54aa",
   "metadata": {},
   "source": [
    "Conclusoes do Modeling :))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
